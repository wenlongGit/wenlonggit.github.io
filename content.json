{"meta":{"title":"识数","subtitle":"“信数据,得永生！\"——未来简史","description":"大数据 + 人工智能 = 业务增长","author":"Alex Zhang","url":"http://wenlonggit.github.io"},"pages":[{"title":"about","date":"2017-02-06T09:36:15.000Z","updated":"2017-03-12T15:12:30.000Z","comments":true,"path":"about/index.html","permalink":"http://wenlonggit.github.io/about/index.html","excerpt":"","text":"关于我： 徘徊着的,在路上的,你要走吗?易碎的,骄傲着,那也曾是你的模样沸腾着的,不安着的,你要去哪?谜一样的,沉默着的,故事你真的在听吗? 你曾经拥有着的一切，转瞬如过眼云烟；你曾经失掉所有方向，回首平凡才是彼岸！ 13.7至今从事于数据挖掘相关工作 坚信坚持写作和坚持跑步一样，坚持下去，才会看到生活的平凡本真 联系我: 本博客主要在于积累整理生活与工作的所思所想。一家之言，难免疏漏！欢迎的大家拍砖指正，可以直接在文章下方评论，也可以通过邮件或微博联系我。 邮箱: wenlong237@gmail.com 微博：未名之殇2013 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/about/index.html"},{"title":"“tags”","date":"2017-02-10T08:35:14.000Z","updated":"2017-02-10T08:35:14.000Z","comments":true,"path":"“tags”/index.html","permalink":"http://wenlonggit.github.io/“tags”/index.html","excerpt":"","text":"本文链接：http://wenlonggit.github.io/“tags”/index.html"},{"title":"","date":"2017-03-12T16:39:29.000Z","updated":"2017-03-12T16:39:29.000Z","comments":true,"path":"404.html","permalink":"http://wenlonggit.github.io/404.html","excerpt":"","text":"html ___ ![](https://licensebuttons.net/l/by/2.5/cn/88x31.png)本作品采用[知识共享署名 2.5 中国大陆许可协议](http://creativecommons.org/licenses/by/2.5/cn/)进行许可，欢迎转载，但转载请注明来自[识数大数据](http://shidata.com)，并保持转载后文章内容的完整。本人保留所有版权相关权利。 本文链接：http://wenlonggit.github.io/404.html"}],"posts":[{"title":"Anaconda入门","slug":"anaconda-turorial","date":"2019-06-02T06:55:03.000Z","updated":"2020-03-02T11:59:28.000Z","comments":true,"path":"2019/06/02/anaconda-turorial/","link":"","permalink":"http://wenlonggit.github.io/2019/06/02/anaconda-turorial/","excerpt":"","text":"一、Anaconda介绍Anaconda是专注于数据分析的Python发行版本，包含了conda、Python等190多个科学包及其依赖项。 Anaconda通过管理工具包、开发环境、Python版本，大大简化了你的工作流程。不仅可以方便地安装、更新、卸载工具包，而且安装时能自动安装相应的依赖包，同时还能使用不同的虚拟环境隔离不同要求的项目。 在 Anaconda 官网中是这么宣传自己的：适用于企业级大数据分析的Python工具。其包含了720多个数据科学相关的开源包，在数据可视化、机器学习、深度学习等多方面都有涉及。不仅可以做数据分析，甚至可以用在大数据和人工智能领域。 Anaconda Navigator ：用于管理工具包和环境的图形用户界面，后续涉及的众多管理命令也可以在 Navigator 中手工实现。 Jupyter notebook ：基于web的交互式计算环境，可以编辑易于人们阅读的文档，用于展示数据分析的过程。 qtconsole ：一个可执行 IPython 的仿终端图形界面程序，相比 Python Shell 界面，qtconsole 可以直接显示代码生成的图形，实现多行代码输入执行，以及内置许多有用的功能和函数。 spyder ：一个使用Python语言、跨平台的、科学运算集成开发环境。 安装之后，在命令行中输入命令，对所有的工具包进行升级 1conda upgrade —all 二、管理python虚拟环境创建虚拟环境 1conda create -n env_name list of packages 其中 -n 代表 name，env_name 是需要创建的环境名称，list of packages 则是列出在新环境中需要安装的工具包。 例如，当我安装了 Python3 版本的 Anaconda 后，默认的 root 环境自然是 Python3，但是我还需要创建一个 Python 2 的环境来运行旧版本的 Python 代码，最好还安装了 pandas 包，于是我们运行以下命令来创建： 1conda create -n py2 python=2.7 pandas 细心的你一定会发现，py2 环境中不仅安装了 pandas，还安装了 numpy 等一系列 packages，这就是使用 conda 的方便之处，它会自动为你安装相应的依赖包，而不需要你一个个手动安装。 进入名为 env_name 的环境： 1source activate env_name 退出当前环境： 1source deactivate 另外注意，在 Windows 系统中，使用 activate env_name 和 deactivate 来进入和退出某个环境。 删除名为 env_name 的环境： 1conda env remove -n env_name 显示所有的环境： 1conda env list 当分享代码的时候，同时也需要将运行环境分享给大家，执行如下命令可以将当前环境下的 package 信息存入名为 environment 的 YAML 文件中。 1conda env export &gt; environment.yaml 同样，当执行他人的代码时，也需要配置相应的环境。这时你可以用对方分享的 YAML 文件来创建一摸一样的运行环境。 1conda env create -f environment.yaml 三、管理python安装包安装一个 package 1conda install package_name 这里 package_name 是需要安装包的名称。你也可以同时安装多个包，比如同时安装numpy 、scipy 和 pandas，则执行如下命令： 1conda install numpy scipy pandas 你也可以指定安装的版本，比如安装 1.1 版本的 numpy ： 1conda install numpy=1.10 移除一个 package： 1conda remove package_name 升级 package 版本： 1conda update package_name 查看所有的 packages： 1conda list 如果你记不清 package 的具体名称，也可以进行模糊查询： 1conda search search_term 四、连接pycharm打开PyCharm，进入preference; 选择Project Interprete; 点击右边齿轮，选择 Add Local; 在弹出的菜单框中，找到Anaconda的安装目录，我的是 Users/$USERNAME/anaconda2/; 选择python.app/MacOS/python; 点击OK即可； 到此为止PyCharm 配置 Anaconda环境就已经完成了，耐心等待加载，就会看到常用第三方库配置到了PyCharm中，由于Anaconda中配置了常用的第三方库，就省去了常用的pip安装。 五、conda vs pyenvpyenv 是一款特别好用的Python版本管理器，可以在同一台电脑上不同的目录里分别运行不同版本的Python， 并且互不影响，安装的包也互不影响。github项目地址：https://github.com/yyuu/pyenv pyenv-virtualenv 是pyenv的一个plugin，可以用来创建基于不同Python版本的干净的虚拟环境。github项目地址：https://github.com/yyuu/pyenv-virtualenv conda与pyenv-virtualenv的区别 conda可管理python包以及python虚拟环境 pyenv-virtualenv是pyenv（python版本管理软件）的插件，可以管理python虚拟环境 conda除了可以安装python包外，还可以安装c++或R等其他语言的包 可以简单认为 conda = pip（python安装包管理）+ pyenv-virtualenv 但conda只能安装管理1500多种安装包，pip则有15000多种安装包 conda有两个软件版本，\b\bAnaconda与miniconda 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2019/06/02/anaconda-turorial/","categories":[{"name":"开发环境和工具","slug":"开发环境和工具","permalink":"http://wenlonggit.github.io/categories/开发环境和工具/"}],"tags":[{"name":"python","slug":"python","permalink":"http://wenlonggit.github.io/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"http://wenlonggit.github.io/tags/anaconda/"},{"name":"pyenv","slug":"pyenv","permalink":"http://wenlonggit.github.io/tags/pyenv/"}]},{"title":"wireshark快速入门","slug":"wireshark-tutorial","date":"2019-05-13T13:56:21.000Z","updated":"2019-05-14T02:15:43.000Z","comments":true,"path":"2019/05/13/wireshark-tutorial/","link":"","permalink":"http://wenlonggit.github.io/2019/05/13/wireshark-tutorial/","excerpt":"","text":"1、wireshark介绍wireshark是一个非常流行的网络封包分析软件。功能十分强大，可以截取各种网络封包，显示网络封包的详细信息。 wireshark还是开源软件，可以放心使用。官网地址：https://www.wireshark.org/ 使用wireshark必须了解网络协议，否则很难看懂。 wireshark只能查看封包，不能修改封包内容，或者发送封包。 wireshark能获取HTTP，也能获取HTTPS，但不能解密HTTPS，所以wireshark看不懂HTTPS中的内容。 除了HTTP/HTTPS，wireshark还支持获取TCP/UDP协议。 2、使用wireshark抓包wireshark是捕获机器上的某一块王网卡的网络包，当你机器上有多块网卡的时候，要先选择网卡。 点击【捕获】—&gt;【选项】，在弹出的对话框中，选择一块网卡，然后店家开始按钮，开始抓包。 3、wireshark窗口介绍wireshark版本v2.0.4，如下图： 1、菜单 程序上方的8个菜单项用于对Wireshark进行配置： “File”（文件）打开或保存捕获的信息。 “Edit” （编辑）查找或标记封包。进行全局设置。 “View”（查看）设置Wireshark的视图。 “Go” （转到）跳转到捕获的数据。 “Capture”（捕获）设置捕捉过滤器并开始捕捉。 “Analyze”（分析）设置分析选项。 “Statistics” （统计） 查看Wireshark的统计信息。 “Help” （帮助）查看本地或者在线支持。 2、显示过滤器 显示过滤器用于查找捕捉记录中的内容。 使用过滤是非常重要的， 初学者使用wireshark时，将会得到大量的冗余信息，在几千甚至几万条记录中，以至于很难找到自己需要的部分。搞得晕头转向。 过滤器会帮助我们在大量的数据中迅速找到我们需要的信息。 过滤器有两种，一种是显示过滤器，就是主界面上那个，用来在捕获的记录中找到所需要的记录；一种是捕获过滤器，用来过滤捕获的封包，以免捕获太多的记录。 在Capture -&gt; Capture Filters 中设置 3、封包列表 封包列表中显示所有已经捕获的封包。 在这里您可以看到发送或接收方的MAC/IP地址，TCP/UDP端口号，协议或者封包的内容。 封包列表的面板中显示，编号，时间戳，源地址，目标地址，协议，长度，以及封包信息。 你可以看到不同的协议用了不同的颜色显示。 如果捕获的是一个OSI layer 2的封包，您在Source（来源）和Destination（目的地）列中看到的将是MAC地址， 当然，此时Port（端口）列将会为空。如果捕获的是一个OSI layer 3或者更高层的封包，您在Source（来源）和Destination（目的地）列中看到的将是IP地址。Port（端口）列仅会在这个封包属于第4或者更高层时才会显示。 您可以在这里添加/删除列或者改变各列的颜色：Edit menu -&gt; Preferences 4、封包详细信息 这里显示的是在封包列表中被选中项目的详细信息。信息按照不同的OSI layer进行了分组，您可以展开每个项目查看。 这个面板是我们最重要的，用来查看协议中的每一个字段。 当我们截获http协议的数据包时，各行信息分别为 Frame: 物理层的数据帧概况 Ethernet II: 数据链路层以太网帧头部信息 Internet Protocol Version 4: 网络层IP包头部信息 Transmission Control Protocol: 传输层T的数据段头部信息，此处是TCP Hypertext Transfer Protocol: 应用层的信息，此处是HTTP协议 5、封包16进制数据 “解析器”在Wireshark中也被叫做“16进制数据查看面板”。这里显示的内容与“封包详细信息”中相同，只是改为以16进制的格式表述。 4、wireshark常用技巧 保存过滤 在Filter栏上，填好Filter的表达式后，点击Save按钮， 取个名字。比如”Filter 102″，Filter栏上就多了个”Filter 102″ 的按钮。 过滤表达式的规则 表达式规则 协议过滤：比如TCP，只显示TCP协议。 IP 过滤：比如 ip.src ==192.168.1.102 （显示源地址为192.168.1.102），ip.dst==192.168.1.102（ 目标地址为192.168.1.102） 端口过滤：tcp.port ==80, 端口为80的，tcp.srcport == 80, 只显示TCP协议的愿端口为80的 Http模式过滤：http.request.method==”GET”, 只显示HTTP GET方法的。 逻辑运算符为 AND/ OR 捕获过滤器 告诉wireshark我们只需要捕获满足什么条件的包，而不满足条件的包则不需要捕获。 由于捕获过滤器是在wireshark在捕获过程中采用的，所以捕获过滤器的过滤条件最多局限在传输层的协议，也就是可以通过ip和端口指定。 语法： Protocol Direction Host(s) Value LogicalOperations Other expression 例子：tcp dst 10.1.1.1 80 or tcp dst 10.2.2.2 3128 解释：Protocol表示协议，Direction表示方向，Host指定IP地址，Value一般指定端口，可以使用逻辑操作连接其他的表达式来生成复合表达式。例如： tcp dst port 8888 捕获目的tcp端口为8888的包 ip src host 10.1.1.1 捕获来源地址为10.1.1.1的包 host 10.1.1.1捕获目的或者来源地址为10.1.1.1的包 not icmp捕获除了icmp包的所有包 有关更多的捕获过滤器请参考wireshark的文档 显示过滤器 在显示过滤器的规则中wireshark可以根据每一层的协议进行筛选，例如网络层(IP,ARP,ICMP等协议)，传输层(TCP,UDP)等协议，应用层(HTTP，RTMP，RTSP等协议)，各层协议的字段可以通过逻辑与(&amp;&amp;，and)，或(||，or)，非(|，not)等运算符连接成复合表达式进行过滤。 该过滤器是在已经抓到的包中筛选出自己想分析的数据包，也就是说该过滤器是在捕获工作已经完成之后做的，其数据基础就是已经捕获到的那些数据包，该过滤器支持的协议就是wireshark能够识别的所有协议，由于是在已经捕获下来的包中进行筛选，所以该过滤器中的条件表达式可以支持所有的上层协议，其筛选条件也可以根据每个协议的不同部分进行筛选。 下面以HTTP协议为例子： http.request查看所有http请求的数据包(包括GET，POST等等的请求，只要是web请求都算) http.request.method==&quot;post&quot;所有POST请求的数据包 http.request.uri contains &quot;.jpg&quot;所有请求的URL中包含字符串”.jpg”的包 httop.response.code==200所有http响应状态码为200的包 可以看到HTTP协议有很多字段提供筛选 wireshark还支持matches操作符，进行正则筛选 http.request.uri matches &quot;.*/mvc/.*aspx\\?.*$&quot;查找所有http请求URL中包含 /mvc/字符串并且请求的是带参数的aspx页面的包 关于各种协议的字段文档可以查阅wireshark的文档，都有详细说明，包括协议的每个字段部分的含义。其实对于做app或者是web开发的来说常见的http筛选字段已经足够用了 此外wireshark除了可以根据协议的每个字段的内容值进行筛选之外，还可以指定数据包中的第几个字节的二进制数据值进行筛选 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2019/05/13/wireshark-tutorial/","categories":[{"name":"开发环境和工具","slug":"开发环境和工具","permalink":"http://wenlonggit.github.io/categories/开发环境和工具/"}],"tags":[{"name":"wireshark","slug":"wireshark","permalink":"http://wenlonggit.github.io/tags/wireshark/"},{"name":"入门","slug":"入门","permalink":"http://wenlonggit.github.io/tags/入门/"}]},{"title":"数学基础——浅谈似然","slug":"likelihood","date":"2018-01-19T11:08:03.000Z","updated":"2019-04-29T07:28:17.000Z","comments":true,"path":"2018/01/19/likelihood/","link":"","permalink":"http://wenlonggit.github.io/2018/01/19/likelihood/","excerpt":"","text":"本次学习预期收益： 似然概念，与概率的区别 极大似然求解过程和原理 1、预备知识1.1 概率公理基于测度论的概率公理：概率是一种测度。 第一公理：任一事件的概率都可以用0到 1区间上的一个实数来表示 第二公理：归一原则：事件空间的概率值为1 第三公理：概率加法原则：不相交子集的并的事件集合的概率为那些子集的概率的和 1.2 概率密度函数PDF（Probability Density Function）定义： X是一个连续型随机变量——X的PDF就是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数 性质： 随机变量的取值落在某个区域之内的概率 == 概率密度函数在这个区域上的积分 累积分布函数是概率密度函数的积分 连续型的随机变量取值在任意一点的概率都是0，概率P=0并不是不可能事件 1.3 概率质量函数PMF（probability mass function）定义： X是离散随机变量——X的PMF为X在各特定取值上的概率 PDF与PMF区别： 概率质量函数是对离散随机变量定义的，本身代表该值的概率； 概率密度函数是对连续随机变量定义的，本身不是概率，只有对连续随机变量的概率密度函数在某区间内进行积分后才是概率 2、什么是似然（likelihood）似然翻译自英文likelihood —— 可能性。似：像；然：这样。似然——像这样。。。似然，感觉我穿越到了清末。。。咳咳，为了便于理解，还是忘记似然，跟我一起在心里默念三遍——似然==可能性！！！似然==可能性！！！似然==可能性！！！ 2.1、似然与概率在英语语境中，likelihood与probability是一样的意思，都表示机会。不仅在英语语境中，在汉语中，似然，或然，概率也都表示某事发生的可能性。只是在数学的概率统计科学中，似然和概率有明确的区分。 概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果。似然则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。 但要注意的是统计学中的概率要满足概率公里，似然不满足概率公理。 3、似然函数似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。 给出似然函数的定义公式时，要考虑随机变量的概率分布类型。 如果是离线分布类型：给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率： L(θ|x)=P(X=x;θ)需要注意的是，上式中的参数θ并不是随机变量。 如果是连续型分布：假定一个关于参数θ、具有连续概率密度函数f的随机变量X，则在给定X的输出x时，参数θ的似然函数可表示为 L(θ|x)=f_θ(x) 似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大。对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。 在某种意义下，似然函数可以理解为条件概率的逆反。 似然函数乘以一个正的常数之后仍然是似然函数，其取值并不需要满足归一化条件 3.1、对数似然函数涉及到似然函数的许多应用中，更方便的是使用似然函数的自然对数形式，即“对数似然函数”。求解一个函数的极大化往往需要求解该函数的关于未知参数的偏导数。因为自然对数log是一个连续且在似然函数的值域内严格递增的上凸函数。求对数通常能够一定程度上简化运算。所以对数似然函数常用在最大似然估计及相关领域中。例如：求解Gamma分布中参数的最大似然估计问题： 4、极大似然估计MLE（maximum likelihood estimation） MLE是一种广泛应用的参数估计方法（模型已定，参数未知，需要估计）。该方法最初由德国数学家高斯提出，但这个方法通常被归功于英国统计学家罗纳德·菲舍尔。其定义如下：给定一个概率分布D，已知其概率密度函数（连续分布）或概率质量函数（离散分布）为 f_D，以及一个分布参数 θ，我们可以从这个分布中抽出一个具有 n个值的采样 x 利用f_D计算出其似然函数： lik(θ|x_1,... ,x_n)=f_θ(x_1,... ,x_n) 若 D是离散分布,f_θ 即是在参数为 θ 时观测到这一采样的概率。若 D是连续分布， θ则为 X_1, X_2,…, X_n联合分布的概率密度函数在观测值处的取值。一旦我们获得 X_1, X_2,…, X_n，我们就能求得一个关于θ 的估计。最大似然估计会寻找关于 θ 的最可能的值（即，在所有可能的θ 取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在θ的所有可能取值中寻找一个值使得似然函数取到最大值。这个使可能性最大的θ值即称为θ的最大似然估计。由定义，最大似然估计是样本的函数。 4.1 极大似然估计例子：4.1.1 离散分布，离线参数空间 4.1.2 离散分布，连续参数空间 4.1.3 连续分布，连续参数空间见第3节中伽马分布的例子 极大似然估计的一般求解步骤为：（1）写出似然函数 （一般是概率密度或质量函数）（2）取平均对数似然函数 （最大化一个似然函数同最大化它的自然对数是等价的）（3）求解似然估计值 5、似然比5.1、最大似然比检验似然比检验是利用似然函数来检测某个假设（或限制）是否有效的一种检验。一般情况下，要检测某个附加的参数限制是否是正确的，可以将加入附加限制条件的较复杂模型的似然函数最大值与之前的较简单模型的似然函数最大值进行比较。如果参数限制是正确的，那么加入这样一个参数应当不会造成似然函数最大值的大幅变动。一般使用两者的比例来进行比较，这个比值是卡方分配。 5、参考资料 如何理解似然函数? 似然函数-维基百科 最大似然估计-维基百科​ 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2018/01/19/likelihood/","categories":[{"name":"数学基础","slug":"数学基础","permalink":"http://wenlonggit.github.io/categories/数学基础/"}],"tags":[{"name":"概率统计","slug":"概率统计","permalink":"http://wenlonggit.github.io/tags/概率统计/"},{"name":"MLE","slug":"MLE","permalink":"http://wenlonggit.github.io/tags/MLE/"},{"name":"极大似然估计","slug":"极大似然估计","permalink":"http://wenlonggit.github.io/tags/极大似然估计/"},{"name":"对数似然函数","slug":"对数似然函数","permalink":"http://wenlonggit.github.io/tags/对数似然函数/"}]},{"title":"Spark-Python脚本快速上手","slug":"pyspark-tutorial","date":"2017-09-20T09:00:21.000Z","updated":"2019-04-29T07:28:55.000Z","comments":true,"path":"2017/09/20/pyspark-tutorial/","link":"","permalink":"http://wenlonggit.github.io/2017/09/20/pyspark-tutorial/","excerpt":"","text":"使用python连接spark开发非交互式的独立程序需要自行初始化SparkContext 运行python脚本需要使用spark自带的bin/spark-submit脚本运行。spark-submit会帮助引入python程序的spark依赖，并且已经配置好了调用spark的PythonAPI的环境。 运行python脚本 1bin/spark-submit my_script.py 初始化Spark 创建SparkConf对象配置应用 初始化SparkContext 123456789from pyspark import SparkConf, SparkContext conf = SparkConf().setMaster(\"yarn\").setAppName(\"My test\") # 集群URL 应用名sc = SparkContext(conf = conf)### 创建RDD，处理RDD#退出sc.stop() # sys.exit() System.exit(0) 调用PythonAPI的简单实例——SimpleApp.py 12345678910from pyspark import SparkContextlogFile = os.path.normpath(\"file:///home/hadoop/spark/README.md\") # Should be some file on your systemsc = SparkContext(\"local\", \"Simple App\")logData = sc.textFile(logFile).cache()numAs = logData.filter(lambda s: 'a' in s).count()numBs = logData.filter(lambda s: 'b' in s).count()print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs)) word-count demo 123456789101112131415161718192021222324252627from __future__ import print_functionimport sysfrom operator import addfrom pyspark.sql import SparkSessionif __name__ == \"__main__\": if len(sys.argv) != 2: print(\"Usage: wordcount &lt;file&gt;\", file=sys.stderr) exit(-1) spark = SparkSession\\ .builder\\ .appName(\"PythonWordCount\")\\ .getOrCreate() lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]) counts = lines.flatMap(lambda x: x.split(' ')) \\ .map(lambda x: (x, 1)) \\ .reduceByKey(add) output = counts.collect() for (word, count) in output: print(\"%s: %i\" % (word, count)) spark.stop() Python 入门参考： Python-入门指南 The Python Standard Library Reference The Python Language Reference 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/09/20/pyspark-tutorial/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://wenlonggit.github.io/categories/大数据/"}],"tags":[{"name":"python","slug":"python","permalink":"http://wenlonggit.github.io/tags/python/"},{"name":"spark","slug":"spark","permalink":"http://wenlonggit.github.io/tags/spark/"},{"name":"pyspark","slug":"pyspark","permalink":"http://wenlonggit.github.io/tags/pyspark/"}]},{"title":"Spark Streaming 编程指南&快速入门","slug":"sparkstreaming-tutorial","date":"2017-09-20T09:00:21.000Z","updated":"2019-04-29T07:29:16.000Z","comments":true,"path":"2017/09/20/sparkstreaming-tutorial/","link":"","permalink":"http://wenlonggit.github.io/2017/09/20/sparkstreaming-tutorial/","excerpt":"","text":"检查项 [x] 依赖链接（Linking） [x] 初始化 StreamingContext [x] 离散流（DStreams） [x] DStreams 输入源与接收器（Receivers） [x] DStreams 转化（Transformations） [x] DStreams的输出操作 [x] DataFrame和SQL操作 [x] MLlib 操作 [x] 缓存/持久化（Caching / Persistence） [ ] 检查点 （Checkpointing） [ ] 累加器（Accumulators），广播变量（Broadcast Variables），检查点（Checkpoints） [ ] 应用程序的部署 [ ] 应用程序的监控 说明: 以下为参考SparkStreaming编程指南，结合实际项目需要，边学习边记录快速入门的要义。 使用的各软件版本为： Spark 2.0.1 Python 2.6.6 Kafka 0.9.0.1 基本概念依赖链接（Linking）​ 对于使用scala或者java语言的Spark Streaming项目，需要添加相应的依赖： 12345678// Maven &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt;// SBTlibraryDependencies += &quot;org.apache.spark&quot; % &quot;spark-streaming_2.11&quot; % &quot;2.1.0&quot; ​ 对于使用Kafka、Flume等第三方软件做数据输入源的，由于SparkStreaming核心API中不包含第三方的接口，也需要在项目中引入依赖包，通常为spark-streaming-xyz_2.11 ，常见的需要引入的依赖包如下： 输入源 依赖包 Kafka spark-streaming-kafka-0-8_2.11 Flume spark-streaming-flume_2.11 初始化 StreamingContext​ 要初始化一个SaprkStreaming应用程序，需要构建一个StreamingContext 对象，它是SparkStreaming程序所有功能的主入口。 StreamingContext可以通过SparkContext来创建 12345from pyspark import SparkContextfrom pyspark.streaming import StreamingContextsc = SparkContext(master, appName)ssc = StreamingContext(sc, 1) 其中， master 是以下枚举之一：Spark, Mesos or YARN cluster URL, 或者填写 local[*] 代表以本地模式运行。由于我们的集群是运行在yarn之上的。这里填写yarn。该选项也可以不填，在提交spark-submit程序的时候，通过参数 —master yarn来指定。 appName是Spark程序的运行时候再web ui上展示的名字。 1 为批处理间隔（batch interval），单位是秒。这个间隔要根据Spark应用程序与集群资源的延迟要求设置。 ​初始化完毕StreamingContext后，要完成一个完成Streaming程序还需要以下几步： 通过创建inputDStreams来定义输入源（input sources） 2.通过对DStreams应用变换（transformation）和输出（output）操作来定义流计算（streaming computations）。 3.使用streamingContext.start()开始接收并处理数据 4.使用streamingContext.awaitTermination()等待数据处理结束(包括手动结束以及任何错误导致的结束 ) 5.处理程序也可以通过streamingContext.stop()来手动结束 主要注意的几点： 一旦context被started，其他新的流式计算（streaming computation）就不能在此context种创建或者添加。 一旦context被stopped，此context就不能被重启（restarted) 一个JVM中，只能同时活跃（active）一个streamingContext StreamingContext中的stop()也会停止SparkContext，如果只想stop StreamingContext，可以将stopSparkContext的可选参数设置为false 一个SparkContext可以重复使用来创建多个StreamingContexts。前提是在一个新的StreamingContext创建之前的StreamingContext必须已经stopped（SparkContext不能停） 离散流（DStreams）​ Discretized Stream是Streaming的一个基本的抽象概念。它表示连续一个连续的数据流，可以是从数据源接收的输入数据流，也可以是从输入流中通过转换操作产生的处理数据流。在Spark内，DStreams代表一系列连续的RDDS（RDD是Spark对不变的，分布式数据集的抽象），DStream中的每个RDD都包含了一个确定批处理间隔的数据。具体见下图： ​ 对DStream应用的任何操作都转换为基础RDD上的操作。例如，在将行转换为字的数据流的demo中，对行DStream中的每个RDD应用flatMap操作以生成字DStream的RDD。如下图所示: DStreams 输入源与接收器（Receivers）​ Input DStreams 是指从数据流源接收的输入数据流。每个输入DStream（文件流除外）与Receiver 对象相关联，该对象从数据源接收数据并将其存储在Spark的内存中进行处理。 ​ SparkStreaming提供了两种内置的数据流源： 基础数据源：可通过StreamingContext API直接使用的数据源，例如，文件系统以及socket连接。 高级数据源：比如 Kafka, Flume, Kinesis之类的数据源。需要通过额外的工具类连接，添加额外的依赖。 ​需要注意的是，如果需要在流式应用中并行输入多个数据源，则需要创建多个input Dstreams，这样就能创建能够接收多个数据源的多个receivers。但请注意，Spark worker / executor是一个长期运行的任务，因此它需要占用分配给Spark Streaming应用程序的核（cores)。因此，需要留足够的核数（cores，如果是本地模式，需要留足够的线程数）来保证应用运行receivers以及处理接收到的数据。 需要谨记的点： 当在本地运行Spark Streaming程序时，不要使用“local”或“local [1]”作为master URL。这两个都意味着只有一个线程将用于本地运行任务。如果使用基于接收器（例如套接字（sockets），Kafka，Flume等）的输入DStream，则单个线程将用于运行接收器，没有线程用于处理接收的数据。 同理，在集群上运行，分配给Spark Streaming应用程序的核数必须大于接收器数量。否则应用只能接收数据，不能处理数据。 基础数据源（basic sources）​ 在快速开始的demo中，我们使用 ssc.socketTextStream(...) 创建了一个从TCP socket连接中接收数据的DStream文本数据流。除了sockets，StreamingContext API也提供了从文件输入源创建DStreams的方法。 文件流（File Streams） : 从和HDFS API兼容的任何文件系统（HDFS，S3，NFS等）中的文件读取数据，可以按照如下方式创建DStream： 1streamingContext.textFileStream(dataDirectory) ​ Spark Streaming 会持续监控任何在dataDirectory文件夹中创建的文件（但不支持监控dataDirectory目录中的嵌套子目录中创建的文件），需要注意： 文件中的数据必须是相同的格式 必须是在dataDirectory目录中，以原子（atomically）方式移动（moving）或重名到此目录的文件 文件一旦进入dataDirectory目录，就不能改动了。因此，如果文件中仍然有持续的追加数据，新的数据将不会被StreamingContext读取到 对于简单的text文件，直接调用streamingContext.textFileStream(datadirectory)即可。由于文件流（file streams）不需要运行receiver，因此不需要给streaming程序分配核数(cores) 目前Python API中支持textFileStream, 还不支持fileStrem 自定义接收器的流（Streams based on Custom Receivers）：可以通过自定义接收器接收的数据流创建DStreams，具体要参考自定义接收器编程指南 RDD队列作为流（Queue of RDDs as a Stream）： 如果我们要用测试数据测试一个Spark Streaming应用程序，可以使用streamingContext.queueStream(queueOfRDDs)创建一个基于RDDs序列的DStreams。这样，每个被push入queue的RDD都会被视为DStream的一批数据，并以流式方式处理他们 高级数据源（Advanced Source）​ 自Spark2.0.1开始，Spark Python API开始支持Kafka、Flume、Kinesis等高级数据源 ​ 高级源的需要一些非Spark自带的库，以及一些比较复杂的第三方依赖。可以通过前面介绍的依赖链接来引入第三方依赖。 ​ 高级源不可直接在Saprk shell中使用，所以在Spark shell中无法直接测试高级源。如果想要在Spark shell中直接测试高级源，需要将这些源以及相关依赖的jar包从maven下载下来，并加入到classpath中。 ​ 高级源的版本兼容情况如下： Kafka Spark Streaming 2.0.1兼容kafka broker 0.8.2.1及以上版本 Flume Spark Streaming2.0.1兼容flume 1.6.0 Kinesis Spark Streaming2.0.1兼容Kinesis client 1.2.1 自定义源（Custom Sources）​ Python API 不支持自定义源 ​ Input DStreams 可以从自定义的数据源中创建，需要实现自定义的receiver，以便从自定义源中接收数据并push给Spark处理。 接收器可靠性（Receiver Reliability）​ Kafka、Flume等数据源支持传输的数据确认被收到。依据数据源的可靠性，如果从Kafka、Flume等可靠的数据源中接收数据，能够保证无论出现什么错误数据都不会丢失，因此，接收器的可靠性可根据源的可靠性分为两种： 可靠的接收器：当数据被接收并被存储到Spark中后，可靠的接收器能够正确的发送确认信息给可靠的源。 不可靠的接收器：不可靠的接收器不会发送确认信息给源。当允许数据丢失情况出现的场景下，可以使用不可靠的接收器。 DStreams 转化操作（Transformations）​ 和RDDs类似，Streaming的转化操作可以修改来自input DStream中的数据。DStreams支持许多基于一般RDD的转化操作，常用的如下： 转化操作 释义 map(func) flatMap(func) filter(func) repartition(numPartitions) union(otherStream) count() reduce(func) countByValue() reduceByKey(func, [numTasks]) join(otherStream, [numTasks]) cogroup(otherStream, [numTasks]) transform(func) updateStateByKey(func) UpdateStateByKey Operation To be edited Transform Operation To be edited Window Operations To be edited Join Operations To be edited DStreams的输出操作 To be edited DataFrame和SQL操作 To be edited MLlib 操作 To be edited 缓存/持久化（Caching / Persistence） To be edited 检查点 （Checkpointing） To be edited 累加器（Accumulators），广播变量（Broadcast Variables），检查点（Checkpoints） To be edited 应用程序的部署 To be edited 应用程序的监控 To be edited 性能调优 Reducing the Batch Processing Times Setting the Right Batch Interval Memory Tuning Fault-tolerance Semantics To be edited 进一步参考1、Kafka集成指南2、Spark Streaming Scala 相关文档 StreamingContext 与 DStream KafkaUtils, FlumeUtils, KinesisUtils3、Spark Streaming Python API文档 StreamingContext DStream KafkaUtils4、相关demo Scala Python5、Spark Streaming 相关论文和视频 论文 视频 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/09/20/sparkstreaming-tutorial/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://wenlonggit.github.io/categories/大数据/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://wenlonggit.github.io/tags/spark/"},{"name":"sparkstreaming","slug":"sparkstreaming","permalink":"http://wenlonggit.github.io/tags/sparkstreaming/"}]},{"title":"SparkShell快速上手","slug":"sparkshell-tutorial","date":"2017-09-20T09:00:21.000Z","updated":"2019-04-29T07:29:10.000Z","comments":true,"path":"2017/09/20/sparkshell-tutorial/","link":"","permalink":"http://wenlonggit.github.io/2017/09/20/sparkshell-tutorial/","excerpt":"","text":"SparkShell是Spark自带的交互式shell 可用来作即时数据分析 SparkShell可用来与分布式存储在许多机器的内存或硬盘上的数据进行交互。而处理过程分发由Spark自动完成 SparkShell Python和Scala版本的实例 SparkShell启动时，默认就已经自动初始化了一个SparkContext——sc Python 版本打开python版本的sparkshell 123bin/pyspark #python版本shellbin/spark-shell #sparkshell Python 行数统计 12345# 创建一个名为lines的RDD，默认读取的是hdfs用户根目录下的文件 # 读取本地文件 sc.textFile(&quot;file:///test/wc.txt&quot;) lines = sc.textFile(&quot;/user/hadoop/test/wc.txt&quot;) # sc —— SparkContext; lines.count() # 统计RDD中元素的个数lines.first() # RDD的第一个元素，README.md中的第一行 Python 筛选单词 12pythonlines = lines.filter(lambda line:&quot;(But,1)&quot; in line)pythonlines.first() Spark python API 与 Python API 速查手册： Spark-Python-API Python-入门指南 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/09/20/sparkshell-tutorial/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://wenlonggit.github.io/categories/大数据/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://wenlonggit.github.io/tags/spark/"},{"name":"shell","slug":"shell","permalink":"http://wenlonggit.github.io/tags/shell/"},{"name":"sparkshell","slug":"sparkshell","permalink":"http://wenlonggit.github.io/tags/sparkshell/"}]},{"title":"python2.6升级到2.7与pip的安装","slug":"py2_6up2_7","date":"2017-09-20T09:00:21.000Z","updated":"2019-04-29T07:28:31.000Z","comments":true,"path":"2017/09/20/py2_6up2_7/","link":"","permalink":"http://wenlonggit.github.io/2017/09/20/py2_6up2_7/","excerpt":"","text":"python2.6升级到2.7与pip的安装1、python2.6升级服务器上面装的centos5.8的镜像，默认安装的是python2.6.8，由于python官方已经停止了2.6版本的更新，很多第三方的python包也只支持2.7以上的python，例如sckit-learn，所以很有必要升级python 1234567891011121314151617181920# 查看当前系统中的 Python 版本，返回 Python 2.6.8 为正常python --versionPython 2.6.8# 检查 CentOS 版本，返回 CentOS release 5.8 (Final) 为正常cat /etc/redhat-releaseCentOS release 5.8 (Final)# 安装所有的开发工具包yum groupinstall -y &quot;Development tools&quot;# 安装其它的必需包yum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel# 下载、编译和安装 Python 2.7.13#wget https://www.python.org/ftp/python/2.7.13/Python-2.7.13.tgztar zxf Python-2.7.13.tgzcd Python-2.7.13./configuremake &amp;&amp; make install (这一步需要root权限)# 查看新的 Python 版本，返回 Python 2.7.13 为正常python --versionPython 2.7.13 2、pip的安装pip的安装方式有几种： 1、 使用脚本安装： 需要下载get-pip.py,然后运行：1sudo -H python get-pip.py 2、使用软件管理器安装 在Linux系统中，pip通常可以在系统的软件管理器中安装，不过通过此方法安装的 一般不会是最新版本的pip. 12345# Debian 和 Ubuntu: sudo apt-get install python-pip# Fedora: sudo yum install python-pip 一般推荐使用第一种方式安装，不过安装过后，可能是由于python的环境变量原因，直接pip，并不生效： 12345[root@10-10-122-164 user_profile]# pipTraceback (most recent call last): File &quot;/usr/bin/pip&quot;, line 7, in ? from pip import mainImportError: No module named pip 但是查到一种方法，说是python自身的环境变量如果没有问题的话，可以用python -m pip来使用pip命令安装软件。例如，我们安装numpy 1234567[root@10-10-122-164 user_profile]# python -m pip install numpyCollecting numpy/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning. SNIMissingWarning Downloading numpy-1.13.1-cp27-cp27m-manylinux1_x86_64.whl (16.6MB) 100% |████████████████████████████████| 16.6MB 17kB/sInstalling collected packages: numpy 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/09/20/py2_6up2_7/","categories":[{"name":"开发环境和工具","slug":"开发环境和工具","permalink":"http://wenlonggit.github.io/categories/开发环境和工具/"}],"tags":[{"name":"python","slug":"python","permalink":"http://wenlonggit.github.io/tags/python/"},{"name":"pip","slug":"pip","permalink":"http://wenlonggit.github.io/tags/pip/"},{"name":"numpy","slug":"numpy","permalink":"http://wenlonggit.github.io/tags/numpy/"}]},{"title":"Spark数据读取与保存快速上手","slug":"spark-datadeal","date":"2017-09-20T09:00:21.000Z","updated":"2019-04-29T07:29:04.000Z","comments":true,"path":"2017/09/20/spark-datadeal/","link":"","permalink":"http://wenlonggit.github.io/2017/09/20/spark-datadeal/","excerpt":"","text":"Spark支持多种输入与输出： 文件格式与文件系统 多种文件系统：HDFS NFS AmazonS3 多种文件格式：本地文件 JSON Sequencefile Parquet CSV Spark SQL中的结构化数据源 JSON ApacheHive 数据库与键值存储 JDBC HBase Elasticsearch Cassandra 1、文件格式Spark会针对文件扩展名称选择对应的处理方式，并且分装好，对用户透明。 1.1 文本文件Spark读取可以读取一个文本文件为一个RDD，文件的每一行是RDD的一个元素。也可以将多个文本文件一次性的读入为一个pair RDD，键是文件名，值是文件内容。 读取文本文件 Spark支持读取给定目录中的所有文件，以及在输入路径中使用通配符，如part-*.txt. 12# 使用 SparkContext的 textFile()函数。文件路径作为函数参数。input = sc.textFile(\"file:///home/hadoop/user/wenlong/text.txt\") 保存本地文件 Spark中可以使用saveAsTextFile()方法接收一个路径，将RDD中的内容都输出到对应的文件目录中。一般情况下，Spark不能控制哪些数输出到哪一个文件，不过有些数据格式可以支持。 1result.saveAsTextFile(\"xxxx\") 1.2 JSON对于半结构化的JSON数据，Spark会把它当做文本文件读入，然后使用JSON解析器来对RDD中的值进行映射操作。也可以直接使用SparkSQL直接读取JSON数据。 读取JSON Spark读取JSON文件时，假设读入的每一行都是一条JSON记录，如果有跨行的JSON数据，需要读入整个文件，然后对每个文件进行解析。在这里我们使用python自带的json库。 12import json data = input.map(lambda x:json.loads(x)) 保存JSON Spark使用第三方JSON库，将字符串RDD转为解析好的JSON数据. 12# 将上面读入的JSON串中的names抽取出，序列化为json文件保存(data.filter(lambda x: x[\"names\"]).map(lambda x: json.dumps(x))).saveAsTextFile(outputFile) 1.3 CSV/TSV文件，逗号分隔符或制表符分隔符 读取CSV 读取CSV/TSV数据与JSON数据类似，都是需要先把文件当做普通文件读取，再对数据进行处理。这里我们使用python自带的csv库。如果恰好CSV文件中所有的字段均没有包含换行符，可以使用textFile()直接读取解析数据。 123456789import csvimport StringIOdef loadRecord(line): \"\"\"解析一行CSV记录\"\"\" input = StringIO.StringIO(line) reader = csv.DicReader(input, fieldnames=[\"name\",\"intresting\"]) return reader.next()input = sc.textFile(inputFile).map(loadRecord) 如果字段中嵌有换行符，则需要读入每个文件，然后解析各段。如果每个文件都很大，则读取和解析可能会有瓶颈问题。 123456def loadRecord(fileNameContents) \"\"\"读取给定文件中的所有记录\"\"\" input = StringIO.StringIO(fileNameContents[1]) reader = csv.DicReader(input, fieldnames=[\"name\",\"intresting\"]) return reader fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecord) 保存CSV 1.4 SequenceFile To be edited 1.5 对象文件 To be edited 2 文件系统 To be edited 2.1 本地文件系统 To be edited 2.2 HDFS To be edited 3 Spark SQL中的结构化数据 To be edited 结构化数据是指由结构信息的数据，数据的所有记录都有一致的字段结构。 一条SQL语句传递给Spark SQL，它会对一个数据源执行查询（选出一些字段，或对一些字段使用一些函数加工），得到由row对象组成的RDD，每个row都有一个get()方法，可以返回一般类型的数据供我们进行类型转换。也有专门的基本类型的get()方法，如：getFloat()、getInt()等。 在Python中，可以使用row[cloumn_number]以及row.column_name来获取元素。 3.1 Apache HIVESpark SQL可以读取Hive支持的任何表。 Spark SQL连接已有Hive的方法如下： 将hive-site.xml复制到Spark的./conf/目录下。 创建HiveContext对象 使用Hive查询语句（HQL）查询 得到以row为对象的RDD形式的数据结果集 123456789101112#!/usr/bin/python# -*- coding: utf-8 -*-from pyspark.sql import HiveContextfrom pyspark import SparkContextsc = SparkContext(\"yarn\", \"Simple App\")hiveCtx = HiveContext(sc)rows = hiveCtx.sql(\"select * from dw.temp_user_info \") rows.write.parquet(\"hdfs://Ucluster/user/hadoop/test/temp\",'overwrite')names = rows.rdd.map(lambda p: p.name).collect()print names 3.2 JSON To be edited 4 数据库 To be edited 4.1 JDBC To be edited 4.2 HBase To be edited 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/09/20/spark-datadeal/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://wenlonggit.github.io/categories/大数据/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://wenlonggit.github.io/tags/spark/"}]},{"title":"Hadoop/hive文件存储格式","slug":"hadoop-snapshot","date":"2017-05-02T11:08:03.000Z","updated":"2019-04-29T07:27:27.000Z","comments":true,"path":"2017/05/02/hadoop-snapshot/","link":"","permalink":"http://wenlonggit.github.io/2017/05/02/hadoop-snapshot/","excerpt":"","text":"1、基于行存储 基于Hadoop系统行存储结构的优点在于快速数据加载和动态负载的高适应能力，这是因为行存储保证了相同记录的所有域都在同一个集群节点，即同一个 HDFS块。不过，行存储的缺点也是显而易见的，例如它不能支持快速查询处理，因为当查询仅仅针对多列表中的少数几列时，它不能跳过不必要的列读取；此 外，由于混合着不同数据值的列，行存储不易获得一个极高的压缩比，即空间利用率不易大幅提高。 HDFS块内行存储的例子： Textfile 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。文本格式除了会占用更多磁盘资源外，对它的解析开销一般会比二进制格式高，因此强烈不建议在生产系统中使用这些格式进行储存。如果需要输出这些格式，请在客户端做相应的转换操作。文本格式经常会用于日志收集，数据库导入，Hive默认配置也是使用文本格式，而且常常容易忘了压缩，所以请确保使用了正确的格式。 1234567891011&gt; create table test1(str STRING) &gt; STORED AS TEXTFILE; OK Time taken: 0.786 seconds #写脚本生成一个随机字符串文件，导入文件： &gt; LOAD DATA LOCAL INPATH '/home/work/data/test.txt' INTO TABLE test1; Copying data from file:/home/work/data/test.txt Copying file: file:/home/work/data/test.txt Loading data to table default.test1 OK Time taken: 0.243 seconds SequenceFile SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。 其具有使用方便、可分割、可压缩的特点。 支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 123456789CREATE EXTERNAL TABLE IF NOT EXISTS da.test_sequencefile( ...)COMMENT 'test_sequencefile'PARTITIONED BY(dt STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\\001'STORED AS SEQUENCEFILE; 2、基于列存储 查询时列存储能够避免读不必要的列， 并且压缩一个列中的相似数据能够达到较高的压缩比。然而，由于元组重构的较高开销，它并不能提供基于Hadoop系统的快速查询处理。列存储不能保证同一 记录的所有域都存储在同一集群节点，行存储的例子中，记录的4个域存储在位于不同节点的3个HDFS块中。因此，记录的重构将导致通过集群节点网络的大 量数据传输。尽管预先分组后，多个列在一起能够减少开销，但是对于高度动态的负载模式，它并不具备很好的适应性。 HDFS块内列存储的例子： 3、基于行列存储RCFile RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分，再垂直划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。需要说明的是，RCFile在map阶段从远端拷贝仍然是拷贝整个数据块，并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列，并跳到需要读取的列， 而是通过扫描每一个row group的头部定义来实现的，但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下，RCFile的性能反而没有SequenceFile高。 RCFile结合行存储查询的快速和列存储节省空间的特点： 首先，RCFile保证同一行的数据位于同一节点，因此元组重构的开销很低； 其次，像列存储一样，RCFile能够利用列维度的数据压缩，并且能跳过不必要的列读取。 HDFS块内RCFile方式存储的例子： RCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。RCFILE文件示例： 123456789CREATE EXTERNAL TABLE IF NOT EXISTS da.test_rcfile( ...)COMMENT 'test_rcfile'PARTITIONED BY(dt STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\\001'STORED AS RCFILE ; 6、ParquetFile 源自于google Dremel系统（可下载论文参阅），Parquet相当于Google Dremel中的数据存储引擎，而Apache顶级开源项目Drill正是Dremel的开源实现。Apache Parquet 最初的设计动机是存储嵌套式数据，比如Protocolbuffer，thrift，json等，将这类数据存储成列式格式，以方便对其高效压缩和编码，且使用更少的IO操作取出需要的数据，这也是Parquet相比于ORC的优势，它能够透明地将Protobuf和thrift类型的数据进行列式存储，在Protobuf和thrift被广泛使用的今天，与parquet进行集成，是一件非容易和自然的事情。 除了上述优势外，相比于ORC, Parquet没有太多其他可圈可点的地方，比如它不支持update操作（数据写成后不可修改），不支持ACID等。 ParquetFile 的详细介绍参考 深入分析Parquet列式存储格式 123456789CREATE EXTERNAL TABLE IF NOT EXISTS da.test_parquet( ...)COMMENT 'test_parquet'PARTITIONED BY(dt STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\\001'STORED AS PARQUET ; 7、ORCFile ORC（OptimizedRC File）存储源自于RC（RecordColumnar File）这种存储格式，RC是一种列式存储引擎，对schema演化（修改schema需要重新生成数据）支持较差，而ORC是对RC改进，但它仍对schema演化支持较差，主要是在压缩编码，查询性能方面做了优化。RC/ORC最初是在Hive中得到使用，最后发展势头不错，独立成一个单独的项目。Hive 1.x版本对事务和update操作的支持，便是基于ORC实现的（其他存储格式暂不支持）。ORC发展到今天，已经具备一些非常高级的feature，比如支持update操作，支持ACID，支持struct，array复杂类型。你可以使用复杂类型构建一个类似于parquet的嵌套式数据架构，但当层数非常多时，写起来非常麻烦和复杂，而parquet提供的schema表达方式更容易表示出多级嵌套的数据类型。 ORCFile的详细介绍参考 ORC 2015: Faster, Better, Smaller 123456789CREATE EXTERNAL TABLE IF NOT EXISTS da.test_orcfile( ...)COMMENT 'test_orcfile'PARTITIONED BY(dt STRING)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\\001'STORED AS ORCFILE ; Parquet与ORC对比 Parquet ORC Apache顶级项目 开源 列式存储引擎 Apache顶级项目 开源 列式存储引擎 主导公司 Twitter/Cloudera Hortonworks 开发语言 Java Java 嵌套式结构 支持多种编码，字典，RLE，delta编码等 支持主流编码，与parquet类似 ACID 不支持 支持 Update操作（delete，update等） 不支持 支持 支持索引（实际上是统计信息） 粗粒度索引，block/group/chunk级别统计信息 粗粒度索引，file/stripe/row级别统计信息，不能精确到列建索引 查询性能 ORC稍高 数据压缩能力 ORC稍高 支持的查询引擎 Drill Impala Hive …. Hive 6、自定义格式 当用户的数据文件格式不能被当前 Hive 所识别的时候，可以自定义文件格式。用户可以通过实现inputformat和 outputformat来自定义输入输出格式 1234&gt; create table test4(str STRING) &gt; stored as &gt; inputformat 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat' &gt; outputformat 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat'; 7、不同格式文件的性能测试 写入性能 12345678910111213141516171819202122232425262728293031323334353637383940414243444546--TextFilecreate table test_textfile like dw.user_action ;set hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;INSERT OVERWRITE table test_textfile PARTITION(dt='2016-09-20')SELECT u_timestamp, u_backtime, u_responsetime, u_host, u_xff, u_status, u_size, u_div, u_dic, u_diu, u_diu2, u_diu3, u_uid, u_startid, u_stepid, u_time, u_mod, u_ac, u_client, u_ver, u_uuid, u_hash, u_xinge, u_token, u_agent, u_method, u_new_activity, u_old_activity, u_key, u_client_module, u_source, u_page, u_position, u_vid, u_type, u_percent, u_rate, u_user_role, u_isnew_user, u_isdownload, u_isonline, u_buffertime, u_action, u_ishigh, u_cdn_source, u_download_start, u_download_stop, u_fail_cdn_source, u_new_cdn_source, u_width, u_height, u_lon, u_lat, u_province, u_city, u_netop, u_nettype, u_sdkversion, u_model, u_device, u_manufacture, u_reverse0, u_reverse1, u_reverse2, u_reverse3, u_reverse4, u_reverse5, u_reverse6, u_reverse7, u_reverse8, u_reverse9, u_bigger_json FROM dw.user_action WHERE dt='2016-09-20';--SquenceFileset hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;set io.seqfile.compression.type=BLOCK;INSERT OVERWRITE table test_sequencefile PARTITION(dt='2016-09-20')SELECT ... FROM dw.user_action WHERE dt='2016-09-20';--RCFileset hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;INSERT OVERWRITE table test_rcfile PARTITION(dt='2016-09-20')SELECT ... FROM dw.user_action WHERE dt='2016-09-20';--ParquetFileset hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;INSERT OVERWRITE table test_parquet PARTITION(dt='2016-09-20')SELECT ... FROM dw.user_action WHERE dt='2016-09-20'; --ORCFileset hive.exec.compress.output=true;set mapred.output.compress=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;INSERT OVERWRITE table test_orcfile PARTITION(dt='2016-09-20')SELECT ... FROM dw.user_action WHERE dt='2016-09-20'; 记录数 120195204源文件大小 58.5 G （textfile 未压缩） 类型 insert耗时(S) 存储空间(G) ORCFile 318.197 15.6 G Parquet 361.207 16.2 G RCFile 232.284 17.1 G Sequence 283.8 47.5 G TextFile 236.187 22.0 G 查询性能 123456789101112131415-- 方案一，测试整行记录的查询效率：select * from dw.user_action where dt='2016-09-20' and u_diu='869436020507709' ; select * from test_textfile where u_diu='869436020507709' ; select * from test_sequencefile where u_diu='869436020507709' ; select * from test_rcfile where u_diu='869436020507709' ; select * from test_parquet where u_diu='869436020507709' ; select * from test_orcfile where u_diu='869436020507709' ; -- 方案二，测试特定列的查询效率：select u_diu,u_xinge from dw.user_action where dt='2016-09-20' and u_diu='869436020507709' ; select u_diu,u_xinge from test_textfile where u_diu='869436020507709' ; select u_diu,u_xinge from test_sequencefile where u_diu='869436020507709' ; select u_diu,u_xinge from test_rcfile where u_diu='869436020507709' ; select u_diu,u_xinge from test_parquet where u_diu='869436020507709' ; select u_diu,u_xinge from test_orcfile where u_diu='869436020507709' ; 文件格式 查询整行记录耗时（S） 查询特定列记录耗时（S） text(未压缩) 77.911 76.12 text 64.373 65.867 sequence 104.282 84.249 rcfile 42.22 36.359 parquet 75.876 37.206 orcfile 57.094 34.228 参考 浅析Hadoop文件格式 Hive文件存储格式的测试比较 Hive文件存储格式 深入分析Parquet列式存储格式 大数据开源列式存储引擎Parquet和ORC Dremel: Interactive Analysis of Web-Scale Datasets ORC 2015: Faster, Better, Smaller 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/05/02/hadoop-snapshot/","categories":[{"name":"大数据","slug":"大数据","permalink":"http://wenlonggit.github.io/categories/大数据/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://wenlonggit.github.io/tags/Hadoop/"},{"name":"hive","slug":"hive","permalink":"http://wenlonggit.github.io/tags/hive/"},{"name":"文件存储格式","slug":"文件存储格式","permalink":"http://wenlonggit.github.io/tags/文件存储格式/"}]},{"title":"机器学习——TopicModel介绍","slug":"ml-topicmodule","date":"2017-05-02T11:08:03.000Z","updated":"2019-04-29T07:28:23.000Z","comments":true,"path":"2017/05/02/ml-topicmodule/","link":"","permalink":"http://wenlonggit.github.io/2017/05/02/ml-topicmodule/","excerpt":"","text":"一、什么是Topic Module在机器学习以及自然语言处理中, topic model是一种能够在一系列文档集中发现抽象的”主题”的统计学模型。主题模型通常被用作在文本中发现隐含的语义结构的文本挖掘(text-mining)工具。 举个直接的例子,对于一个有特定主题的文档,我们通常可以在文档中找到或多或少频繁出现的特定域的单词,比如”狗”和”骨头”通常出现在和狗相关的文档中,而”猫”和”喵喵”通常出现在和猫相关的文档中。而”这”,”是”这类词通常在所有的文档中都会频繁的出现。 一个文档也可能含有多个主题，且以一定的比例呈现。比如，一篇10%的比例是在写“猫”，90%是在写“狗”的文章，其中出现与狗相关的单词的数量应该是猫的9倍。由Topic Mdoel构建的“主题”通常是一系列相似的词。主题模型通过数学框架捕捉到文档的“直觉”语义，根据单词的统计信息决定文档的主题是什么。 主题模型也被称作概率主题模型。它是用于发现泛文本文体的潜在语义结构的统计算法。在信息时代，我们每天遇到的书面材料数量已经超出了我们的处理能力。而主题模型可以帮助我们去组织和洞察理解这些大量的非机构化文本材料。 最初，主题模型被作为文本挖掘工具来检测遗传信息、图片、网络等数据中的有益的结构化信息。 二、Topic Model的发展历史最初，Papadimitriou, Raghavan, Tamaki and Vempala 等人在1998年提出了topic model。Thomas Hofmann于1999年提出了PLSA(probabilistic latent semantic analysis)。目前，应用的最广泛的topic model要属LDA(Latent Dirichlet allocation)，LDA是PLSA的泛化，由David Blei, Andrew Ng, and Michael I. Jordan等大牛于2002年提出。 LDA在文档-主题、主题-单词分布中引入了稀疏狄利克雷先验分布(Dirichlet prior distributions)，文档只有少量的几类主题，而主题通常可以用几个单词来表达。 其他的topic model一般都是LDA的扩展。 三、上下文信息的主题模型主题模型可以包括上下文信息，例如时间戳，作者信息或与文档相关的地理坐标。此外，网络信息(如文档作者之间的社交网络)也可以被建模。 很多学者研究杂志或报纸随时间推移主题的变化趋势。 四、算法原理如图所示，为一个文档-单词矩阵，每列对应一个文档，每行对应一个单词。矩阵中的元素（单元格）中为单词出现在文档中的词频，颜色越深表示词频越高。对该矩阵进行行列转换，最终，主题模型将使用了相似单词的文档归类，同时也把出现在相似文档中的单词归类。归类结果中得到的文档-单词对集合就是“主题”。 参考1 Topic Model WikiPedia 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/05/02/ml-topicmodule/","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wenlonggit.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://wenlonggit.github.io/tags/机器学习/"},{"name":"TopicModel","slug":"TopicModel","permalink":"http://wenlonggit.github.io/tags/TopicModel/"},{"name":"LDA","slug":"LDA","permalink":"http://wenlonggit.github.io/tags/LDA/"},{"name":"文本挖掘","slug":"文本挖掘","permalink":"http://wenlonggit.github.io/tags/文本挖掘/"}]},{"title":"个人博客部署——域名绑定与插件安装","slug":"hexo-install-3","date":"2017-03-13T13:00:21.000Z","updated":"2019-05-03T10:24:57.000Z","comments":true,"path":"2017/03/13/hexo-install-3/","link":"","permalink":"http://wenlonggit.github.io/2017/03/13/hexo-install-3/","excerpt":"","text":"前面两篇个人博客部署系列博文已经完成了博客的整体搭建工作，下面是一些细节工作，有了这些，让你的博客如虎添翼。 加载第三方评论框 加载第三方分享按钮 使用第三方网站统计 添加帖子访问次数统计 使用七牛云图床 自定义关于页面 自定义404页面 添加打赏 绑定域名 网站地图 RSS 网站图标 SEM优化 加载第三方评论框hexo默认集成的是Disqus，国内的话可以使用多说。多说集成非常简单，仅需如下2步：1、用社交账号登录多说，比如用微博账号。2、填写你要集成多说评论框的网站网址，站名，生成唯一的多说调用网址，再填入一个用于找回的邮箱。经过上述两步，会生成一段通用代码。 12345678910111213141516&lt;!-- 多说评论框 start --&gt; &lt;div class=\"ds-thread\" data-thread-key=\"请将此处替换成文章在你的站点中的ID\" data-title=\"请替换成文章的标题\" data-url=\"请替换成文章的网址\"&gt;&lt;/div&gt;&lt;!-- 多说评论框 end --&gt;&lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt;&lt;script type=\"text/javascript\"&gt;var duoshuoQuery = &#123;short_name:\"wenlong237\"&#125;; (function() &#123; var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); &#125;)(); &lt;/script&gt;&lt;!-- 多说公共JS代码 end --&gt; 3、由于使用的是 hueman主题，主题配置中duoshuo已经是可选的配置项了，只需在themes/_config.yml中，将comment选项中默认的disqus:后的默认内容hexo-theme-hueman注释掉，在duoshuo:中填入在第二步中生成的多说域名xx.duoshuo.com中的xx即可。 加载第三方分享插件多说分享插件: 在博客主题的share目录下，新建一个duoshuo.ejs, 我的路径是 themes/hueman/layout/common/share/ 123456789101112131415161718192021222324&lt;div class=\"ds-share\" data-thread-key=\"&lt;%= post.path %&gt;\" data-title=\"&lt;%= post.title %&gt;\" data-images=\"此处请替换为分享时显示的图片的链接地址\" data-content=\"小伙伴们快来点开看看吧\" data-url=\"&lt;%= page.permalink %&gt;\"&gt; &lt;div class=\"ds-share-inline\"&gt; &lt;ul class=\"ds-share-icons-16\"&gt; &lt;li data-toggle=\"ds-share-icons-more\"&gt;&lt;a class=\"ds-more\" href=\"javascript:void(0);\"&gt;分享到：&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a class=\"ds-wechat\" href=\"javascript:void(0);\" data-service=\"wechat\"&gt;微信&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a class=\"ds-weibo\" href=\"javascript:void(0);\" data-service=\"weibo\"&gt;微博&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a class=\"ds-qq\" href=\"javascript:void(0);\" data-service=\"qq\"&gt;QQ&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a class=\"ds-qzone\" href=\"javascript:void(0);\" data-service=\"qzone\"&gt;QQ空间&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a class=\"ds-baidu\" href=\"javascript:void(0);\" data-service=\"baidu\"&gt;百度&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=\"ds-douban\" href=\"javascript:void(0);\" data-service=\"douban\"&gt;豆瓣网&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=\"ds-facebook\" href=\"javascript:void(0);\" data-service=\"facebook\"&gt;Facebook&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a class=\"ds-twitter\" href=\"javascript:void(0);\" data-service=\"twitter\"&gt;Twitter&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;div class=\"ds-share-icons-more\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 然后就可以在主题下的_config.yml中 share的配置选项中添加一项，duoshuo share: duoshuo 添加帖子访问次数统计使用不蒜子添加网站访问量统计： 添加帖子的访问量统计目的是在每篇帖子下面显示帖子的访问量。找到article.ejs，比如我的主题是在 themes/hueman/layout/common/post/article.ejs在&lt;footer class=&quot;article-footer&quot;&gt;之上，添加如下代码块： 12&lt;!-- 不蒜统计 --&gt;&lt;span style=\"display: inline;\" id=\"busuanzi_container_page_pv\"&gt;本文已被访问 &lt;span id=\"busuanzi_value_page_pv\" font=\"微软雅黑\" style=\"color:black\"&gt;&lt;/span&gt; 次，感谢您的来访！&lt;/span&gt; 添加网站总访问量目的是在网站主页显示网站的整体访问量。找到footer.ejs，比如我的主题是在 themes/hueman/layout/common/post/footer.ejs在&lt;p&gt;Powered by ... &lt;/p&gt;的下方，添加如下代码块； 123&lt;!-- 不蒜统计 --&gt;&lt;span style=\"display: inline;\" id=\"busuanzi_container_site_uv\"&gt;本站总访客数 &lt;span id=\"busuanzi_value_site_uv\" font=\"微软雅黑\" style=\"color:white\"&gt;&lt;/span&gt; 位&lt;/span&gt;&lt;span style=\"display: inline;\" id=\"busuanzi_container_site_pv\"&gt;本站总访问量 &lt;span id=\"busuanzi_value_site_pv\" font=\"微软雅黑\" style=\"color:white\"&gt;&lt;/span&gt; 次&lt;/span&gt; 版权说明 在博客根目录下（和 blog/source 同级），新建一个名为 scripts 的文件夹。 在 scripts 文件夹内, 新建一个 AddTail.js 脚本文件，脚本具体内容详如下： 1234567891011121314151617181920212223242526272829303132333435// Filename: AddTail.js// Author: Alex Zhang// Date: 2017/03/10// Based on the script by KUANG Qi: http://kuangqi.me/tricks/append-a-copyright-info-after-every-post/// Add a tail to every post from tail.md// Great for adding copyright infovar fs = require('fs');hexo.extend.filter.register('before_post_render', function(data)&#123;if(data.copyright == false) return data;// Add seperate linedata.content += '\\n___\\n';// Try to read tail.mdtry &#123;var file_content = fs.readFileSync('tail.md');if(file_content &amp;&amp; data.content.length &gt; 50)&#123;data.content += file_content;&#125;&#125; catch (err) &#123;if (err.code !== 'ENOENT') throw err;// No process for ENOENT error&#125;// 添加具体文章链接, 不需要去掉即可var permalink = '\\n本文链接：' + data.permalink;data.content += permalink;return data;&#125;); 在博客根目录下，新建一个 tail.md 文件，里面写想要展示的版本说明内容。示例如下文所示。 12!(https://licensebuttons.net/l/by/2.5/cn/88x31.png)本作品采用[知识共享署名 2.5 中国大陆许可协议](http://creativecommons.org/licenses/by/2.5/cn/)进行许可，欢迎转载，但转载请注明来自[识数大数据](http://shidata.com)，并保持转载后文章内容的完整。本人保留所有版权相关权利。 版权提示信息加载完毕，然后执行hexo clean ； hexo generate 并同步github即可。 添加打赏据说有些主题是直接支持打赏功能的，由于我们用的hueman主题没有打赏功能，所以需要我们动下手脚咯。 编辑主题内的 article.ejs 文件，比如我这里位于 themes/yilia/layout/_partial/article.ejs 在 &lt;div class=&quot;article-content&quot;&gt;...&lt;/div&gt; 的下面，&lt;%- partial(&#39;footer&#39;) %&gt; 的上面插入如下HTML代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;!-- hexo打赏 --&gt;&lt;% if (!post.excerpt || !index)&#123; %&gt; &lt;% if (theme.donate) &#123; %&gt; &lt;!-- css --&gt; &lt;style type=\"text/css\"&gt; .center &#123; text-align: center; &#125; .hidden &#123; display: none; &#125; .donate_bar a.btn_donate&#123; display: inline-block; width: 82px; height: 82px; background: url(\"http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif\") no-repeat; _background: url(\"http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif\") no-repeat; &lt;!-- http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif 因为本 hexo 生成的博客所用的 theme 的 a:hover 带动画效果， 为了在让打赏按钮显示效果正常 而 添加了以下几行 css， 嵌入其它博客时不一定要它们。 --&gt; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s; &lt;!-- /让打赏按钮的效果显示正常 而 添加的几行 css 到此结束 --&gt; &#125; .donate_bar a.btn_donate:hover&#123; background-position: 0px -82px;&#125; .donate_bar .donate_txt &#123; display: block; color: #9d9d9d; font: 14px/2 \"Microsoft Yahei\"; &#125; .bold&#123; font-weight: bold; &#125; &lt;/style&gt; &lt;!-- /css --&gt; &lt;!-- Donate Module --&gt; &lt;div id=\"donate_module\"&gt; &lt;!-- btn_donate &amp; tips --&gt; &lt;div id=\"donate_board\" class=\"donate_bar center\"&gt; &lt;br&gt; ------------------------------------------------------------------------------------------------------------------------------ &lt;br&gt; &lt;a id=\"btn_donate\" class=\"btn_donate\" target=\"_self\" href=\"javascript:;\" title=\"Donate 打赏\"&gt;&lt;/a&gt; &lt;span class=\"donate_txt\"&gt; &lt;%= theme.donate.text %&gt; &lt;/span&gt; &lt;/div&gt; &lt;!-- /btn_donate &amp; tips --&gt; &lt;!-- donate guide --&gt; &lt;div id=\"donate_guide\" class=\"donate_bar center hidden\"&gt; &lt;br&gt; ------------------------------------------------------------------------------------------------------------------------------ &lt;br&gt; &lt;a href=\"&lt;%= theme.donate.wechat %&gt;\" title=\"用微信扫一扫哦~\" class=\"fancybox\" rel=\"article0\"&gt; &lt;img src=\"&lt;%= theme.donate.wechat %&gt;\" title=\"微信打赏 Colin\" height=\"190px\" width=\"auto\"/&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=\"&lt;%= theme.donate.alipay %&gt;\" title=\"用支付宝扫一扫即可~\" class=\"fancybox\" rel=\"article0\"&gt; &lt;img src=\"&lt;%= theme.donate.alipay %&gt;\" title=\"支付宝打赏 Colin\" height=\"190px\" width=\"auto\"/&gt; &lt;/a&gt; &lt;span class=\"donate_txt\"&gt; &lt;%= theme.donate.text %&gt; &lt;/span&gt; &lt;/div&gt; &lt;!-- /donate guide --&gt; &lt;!-- donate script --&gt; &lt;script type=\"text/javascript\"&gt; document.getElementById('btn_donate').onclick = function() &#123; $('#donate_board').addClass('hidden'); $('#donate_guide').removeClass('hidden'); &#125; function donate_on_web()&#123; $('#donate').submit(); &#125; var original_window_onload = window.onload; window.onload = function () &#123; if (original_window_onload) &#123; original_window_onload(); &#125; document.getElementById('donate_board_wdg').className = 'hidden'; &#125; &lt;/script&gt; &lt;!-- /donate script --&gt; &lt;/div&gt; &lt;!-- /Donate Module --&gt; &lt;% &#125; %&gt;&lt;% &#125;%&gt; 最后，在主题的_config.xml中添加一下配置，以便打赏功能生效 123456#打赏donate: enable: true text: Enjoy it ? Donate me ! 欣赏此文？求鼓励，求支持！ wechat: http://image.shidata.com/pay/wechat.jpg alipay: http://image.shidata.com/pay/paypal.jpg 关于页面直接用markdown格式在博客根目录下的source/about文件夹下，新建一个index.md文件，尽情编辑即可。 404页面推荐使用腾讯公益404页面，为公益贡献微薄之力。只需在博客根目录的source/文件夹下新建一个404.html文件。填充以下内容 1234567891011121314&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" /&gt; &lt;meta name=\"robots\" content=\"all\" /&gt; &lt;meta name=\"robots\" content=\"index,follow\"/&gt;&lt;/head&gt;&lt;body&gt;&lt;script type=\"text/javascript\" src=\"http://www.qq.com/404/search_children.js\" charset=\"utf-8\" homePageUrl=\"your site url \" homePageName=\"回到我的主页\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 但要注意，404页面仅对绑定顶级域名的项目才起作用，GitHub默认分配的二级域名是不起作用的。如果没有绑定域名的话是无法创建404 页面，只会跳转到github默认404 page 绑定域名绑定域名先要了解两个概念： A记录A记录是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。CNAME记录CNAME记录，即：别名记录。这种记录允许您将多个名字映射到同一台计算机。 通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”（A记录）。 它同时提供WWW和MAIL服务，为了便于用户访问服务。可以为该计算机设置两个别名（CNAME）：WWW和MAIL。简单来说：A记录：将域名指向一个IPv4地址（例如：10.10.10.10），需要增加A记录CNAME记录：如果将域名指向一个域名，实现与被指向域名相同的访问效果，需要增加CNAME记录 我们只需在购买域名的服务商提供的域名管理界面里，设置域名的CNAME映射到github.io的域名即可。由于我想把博客绑定到购买的域名的二级域名blog下，所以这里设置CNAME时，需要将主机记录填为blog，如果你要绑定根域名的化，填www就好。 然后在github端添加刚刚绑定的域名：在博客根目录的source/文件夹下新建一个名为CNAME的文件，文件填充你要刚刚绑定的域名即可。 使用百度统计先注册百度统计账号。 由于hueman主题默认配置了百度统计为可选项，在主题的_config.yml文件的plugin设置中，将baidu_analytics的key填充接口。baidu_analytics的key可以在注册百度统计后，百度统计给出的js代码块中找到，如下部分1hm.src = \"https://hm.baidu.com/hm.js?xxxxxxxxxxxxxxxx\"; 参考 不蒜统计 多说 打赏 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/03/13/hexo-install-3/","categories":[{"name":"开发环境和工具","slug":"开发环境和工具","permalink":"http://wenlonggit.github.io/categories/开发环境和工具/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://wenlonggit.github.io/tags/博客/"},{"name":"hexo","slug":"hexo","permalink":"http://wenlonggit.github.io/tags/hexo/"}]},{"title":"个人博客部署——部署Hexo到Github Pages","slug":"hexo-install-2","date":"2017-02-12T13:00:21.000Z","updated":"2019-04-29T07:27:51.000Z","comments":true,"path":"2017/02/12/hexo-install-2/","link":"","permalink":"http://wenlonggit.github.io/2017/02/12/hexo-install-2/","excerpt":"","text":"一、部署 github pagesgithub pages 是github出品的一个免费的静态网页托管服务. 可以用来托管个人博客、组织或部门网站、项目介绍网站等。用github pages托管个人博客不像wordpress，需要配置服务器或VPS。github pages不需要配置服务器，不需要数据库，你只需要把你通过hexo生成的静态博客的html文件，通过git命令push到github即可。快速、方面、访问稳定。网站访问的流量、带宽都由github承担，非常适合用来做个人博客。下面少说废话，我们参照github pages官网文档，来快速配置一个免费的属于你自己的网站吧。 1、创建用来托管博客的github仓库——Create a repository 登录GitHub并创建一个名为username.github.io的新仓库，其中username是GitHub上的用户名（或组织名称）。（需要特别注意，username必须是你的github账号的用户名，如果弄错，后面网站将不可访问） 2、克隆刚刚创建的仓库到你的电脑——Clone the repository 在我们上篇博客个人博客部署——hexo安装中创建的hexo/blog目录中，创建一个新的目录 .deoply , 克隆我们刚刚创建的仓库 12cd hexo/blog git clone https://github.com/username/username.github.io.git .deploy/username.github.io # 这里username是上面提到的你的github账号用户名 3、让我们开始创建github pages的hello world 进入上面克隆的目录 username.github.io中，创建一个新的文件 index.html。写入 hello world 使用git add, commit, push 将刚刚创建的 index.html文件同步到你创建的github仓库中。 在浏览器地址栏中贴入 http://username.github.io 即可访问你全新的个人博客网站了。 12345cd username.github.ioecho &quot;Hello World&quot; &gt; index.htmlgit add --allgit commit -m &quot;Initial commit&quot;git push -u origin master 二、部署Hexo到Github Pages参考其他帖子和hexo文档的说法，将hexo部署到github，只需要在配置文件_config.xml中作如下修改： 1234deploy: type: git repo: git@github.com:username/username.github.io.git branch: master 然后在命令行执行下面命令即可完成远程部署。 1hexo -d 但经过实际操作，在我的环境中 hexo d 命令并不能工作的很好，有些时候，一些对hexo整体配置的修改需要通过 hexo d部署才能生效，但是创建的新帖子以及主题级别的配置修改，通过hexo d并没有按照预期工作。所以参考其他帖子我们创建一个部署脚本，通过git命令来同步更新到远程git仓库，了；来实现本地帖子到网站的更新。 123456hexo generatecp -R public/* .deploy/username.github.iocd .deploy/username.github.iogit add -A git commit -m “update”git push origin master 将上述脚本命名为 deploy.sh 放在 hexo/blog文件夹下，以后需要部署更新的时候，直接执行 sh deploy.sh 即可。 三、新建一个帖子并部署到网站经过上面的配置，我们基本大功告成。下面我们一起新建一个帖子并部署到我们的网站上来，看看效果。 1、新建帖子或文章使用hexo new 命令新建帖子，并在Front-matter(用于配置写入设置的文件开头的YAML或JSON块)中配置文章标题，创建时间，标签，目录，以及缩略图等。 1234567891011121314hexo new test.md ## 使用hexo命令新建一个文件名为test.md的帖子文件。一般默认新建在 hexo/blog/source/_post/文件夹下。# front-matter配置---title: test post date: 2017-02-12 21:00:21tags: - test - hexocategories: - test - test_temp_file thumbnail: http://xxxx.jpg--- 需要额外说明几点： 使用hexo new 命令可以新建 帖子（post) 网页（pages) 草稿(draft) ，命令为 hexo new [layout] layout 取值 post，pages，draft 其中之一，也可以通过 修改 _config.yml中default_layout的取值来指定默认的布局。 关于hexo 写作、配置等基本用法可以参考hexo官方文档 写帖子的过程中会用到不少图片，图片的引用可以使用第三方图床，如七牛云等。 关于后续 hexo的使用、图床的使用、主题的切换配置、域名绑定、分享插件、统计插件、评论插件的安装配置等，我将放到三篇博客个人博客部署——优化你的网站中详细阐述 2、部署更新到github pages上面我们新建了一个test帖子，在没有部署之前，上面的改动都是在你的电脑上的。想让你的改动展现在你的网站上，需要部署一下。很简单，直接运行我们前面配置的部署脚本即可 1sh deploy.sh 现在打开 username.github.io， ctrl + R强制刷新下，既可以看到你的新帖子出现在你的网站了。如果发现新的帖子没有出现，或者是新的更新在网站上没有变动，这些现象大部分是由于现浏览器缓存问题。先别着急，打开浏览器设置，清空浏览器缓存后，再刷新下，就可以看到更新了。 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/02/12/hexo-install-2/","categories":[{"name":"开发环境和工具","slug":"开发环境和工具","permalink":"http://wenlonggit.github.io/categories/开发环境和工具/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://wenlonggit.github.io/tags/博客/"},{"name":"hexo","slug":"hexo","permalink":"http://wenlonggit.github.io/tags/hexo/"},{"name":"github pages","slug":"github-pages","permalink":"http://wenlonggit.github.io/tags/github-pages/"}]},{"title":"个人博客部署——hexo安装","slug":"hexo-install","date":"2017-02-10T11:08:03.000Z","updated":"2019-04-29T07:28:08.000Z","comments":true,"path":"2017/02/10/hexo-install/","link":"","permalink":"http://wenlonggit.github.io/2017/02/10/hexo-install/","excerpt":"","text":"一、安装nodejs由于官网下载比较慢，建议直接使用homebrew安装： 1brew install node 安装非常快，安装完成后检查是否安装成功： 1node -v 二、安装hexo 新建hexo目录 1mkdir ./hexo 安装hexo cli 1npm install hexo-cli -g 初始化blog 1hexo init blog 启动blog 12345cd blognpm installhexo ghexo s 三、Hexo常用的命令12345- hexo generate (hexo g) # 生成静态文件，会在当前目录下生成一个新的叫做public的文件夹- hexo server (hexo s) # 启动本地web服务，用于博客的预览- hexo deploy (hexo d) # 部署播客到远端（比如github, heroku等平台）- hexo new &quot;postName&quot; # 新建文章- hexo new page &quot;pageName&quot; # 新建页面 常用简写 1234hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 常用组合 12hexo d -g #生成部署hexo s -g #生成预览 安装的本地环境 12345678910111213141516$hexo hexo -vhexo-cli: 1.0.2os: Darwin 16.0.0 darwin x64http_parser: 2.7.0node: 7.4.0v8: 5.4.500.45uv: 1.10.1zlib: 1.2.8ares: 1.10.1-DEVmodules: 51openssl: 1.0.2jicu: 58.2unicode: 9.0cldr: 30.0.3tz: 2016j 四、Hexo主题设置安装主题 以安装yilia主题为例 123cd ./blog hexo clean `git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia` 启用主题 修改Hexo目录下的_config.yml配置文件中的theme属性，将其设置为yilia。 更新主题 1234cd themes/yiliagit pullhexo g # 生成hexo s # 启动本地web服务器 现在打开http://localhost:4000/ ，会看到我们已经应用了一个新的主题。 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/02/10/hexo-install/","categories":[{"name":"开发环境和工具","slug":"开发环境和工具","permalink":"http://wenlonggit.github.io/categories/开发环境和工具/"}],"tags":[{"name":"博客","slug":"博客","permalink":"http://wenlonggit.github.io/tags/博客/"},{"name":"hexo","slug":"hexo","permalink":"http://wenlonggit.github.io/tags/hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-02-06T07:26:17.000Z","updated":"2019-04-29T07:27:40.000Z","comments":true,"path":"2017/02/06/hello-world/","link":"","permalink":"http://wenlonggit.github.io/2017/02/06/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 本作品采用知识共享署名 2.5 中国大陆许可协议进行许可，欢迎转载，但转载请注明来自识数大数据，并保持转载后文章内容的完整。本人保留所有版权相关权利。本文链接：http://wenlonggit.github.io/2017/02/06/hello-world/","categories":[],"tags":[]}]}